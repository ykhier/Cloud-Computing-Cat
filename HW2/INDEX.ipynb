{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykhier/Cloud-Computing-Cat/blob/main/INDEX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from collections import defaultdict\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Firebase RTDB endpoint where the inverted index is stored.\n",
        "# Using PUT on this URL replaces the whole index each run.\n",
        "FIREBASE_URL_PLANET = \"https://purrform-a5da9-default-rtdb.firebaseio.com/plant_disease_index.json\"\n",
        "\n",
        "def fetch_page(url):\n",
        "    \"\"\"Fetch a Springer page and return a BeautifulSoup object, or None if the request fails.\"\"\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=15)\n",
        "        if response.status_code == 200:\n",
        "            return BeautifulSoup(response.text, \"html.parser\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        # Silent fail so one broken URL does not stop the full pipeline\n",
        "        return None\n",
        "\n",
        "def extract_content(soup):\n",
        "    \"\"\"Extract text from the Abstract and Introduction sections (best-effort, using multiple selectors).\"\"\"\n",
        "    content = \"\"\n",
        "\n",
        "    # Try a few common patterns for the abstract section\n",
        "    for selector in ['div[id*=\"Abs\"]', 'div.abstract', 'section.Abstract']:\n",
        "        elements = soup.select(selector)\n",
        "        for element in elements:\n",
        "            text = element.get_text(strip=True)\n",
        "            if text:\n",
        "                content += \" \" + text\n",
        "                break\n",
        "\n",
        "    # Try a few common patterns for the introduction section\n",
        "    for selector in ['div[id*=\"Sec1\"]', 'div.introduction', 'section.Introduction']:\n",
        "        elements = soup.select(selector)\n",
        "        for element in elements:\n",
        "            text = element.get_text(strip=True)\n",
        "            if text:\n",
        "                content += \" \" + text\n",
        "                break\n",
        "\n",
        "    return content\n",
        "\n",
        "def get_stop_words():\n",
        "    \"\"\"Return a compact stop-word list to reduce noise in keyword extraction.\"\"\"\n",
        "    return {\n",
        "        # Basic English (30 words)\n",
        "        \"the\", \"and\", \"for\", \"with\", \"that\", \"this\", \"are\", \"was\", \"were\", \"from\",\n",
        "        \"have\", \"has\", \"had\", \"but\", \"not\", \"can\", \"may\", \"will\", \"would\", \"could\",\n",
        "        \"they\", \"them\", \"their\", \"these\", \"those\", \"such\", \"very\", \"much\", \"many\", \"some\",\n",
        "\n",
        "        # Academic/Research terms (15 words)\n",
        "        \"study\", \"paper\", \"research\", \"method\", \"methods\", \"result\", \"results\", \"analysis\",\n",
        "        \"show\", \"use\", \"used\", \"based\", \"data\", \"system\", \"approaches\",\n",
        "\n",
        "        # Generic descriptors (10 words)\n",
        "        \"new\", \"different\", \"important\", \"high\", \"low\", \"large\", \"small\", \"good\", \"better\", \"modern\"\n",
        "    }\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Tokenize text, remove stop-words, apply stemming, then return a cleaned list of terms.\"\"\"\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    # Keep only alphabetic words with length >= 3\n",
        "    words = re.findall(r'\\b[a-zA-Z]{3,}\\b', text.lower())\n",
        "    stop_words = get_stop_words()\n",
        "    stemmer = PorterStemmer()\n",
        "\n",
        "    cleaned_words = []\n",
        "    for word in words:\n",
        "        # First pass: remove stop-words before stemming\n",
        "        if word not in stop_words:\n",
        "            stemmed = stemmer.stem(word)\n",
        "\n",
        "            # Second pass: after stemming, remove short/noisy tokens too\n",
        "            if stemmed not in stop_words and len(stemmed) >= 3:\n",
        "                cleaned_words.append(stemmed)\n",
        "            elif len(stemmed) < 3 and word not in stop_words:\n",
        "                cleaned_words.append(word)\n",
        "\n",
        "    return cleaned_words\n",
        "\n",
        "def build_word_graph(words, window_size=4):\n",
        "    \"\"\"Build a co-occurrence graph where each word connects to nearby words inside a sliding window.\"\"\"\n",
        "    graph = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "    # Walk through the token list and connect each word to neighbors around it\n",
        "    for i, word in enumerate(words):\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(words), i + window_size + 1)\n",
        "\n",
        "        for j in range(start, end):\n",
        "            if i != j:\n",
        "                neighbor = words[j]\n",
        "\n",
        "                # Closer words contribute more than far words\n",
        "                distance = abs(i - j)\n",
        "                weight = 1.0 / distance\n",
        "                graph[word][neighbor] += weight\n",
        "\n",
        "    return graph\n",
        "\n",
        "def calculate_textrank_scores(graph, damping=0.85, iterations=20):\n",
        "    \"\"\"Compute TextRank scores over the word graph using power-iteration.\"\"\"\n",
        "    words = list(graph.keys())\n",
        "    if not words:\n",
        "        return {}\n",
        "\n",
        "    # Start all nodes with the same score\n",
        "    scores = {word: 1.0 for word in words}\n",
        "\n",
        "    # Repeat updates a fixed number of times for stability\n",
        "    for iteration in range(iterations):\n",
        "        new_scores = {}\n",
        "\n",
        "        for word in words:\n",
        "            # Base probability (random jump)\n",
        "            score = 1.0 - damping\n",
        "\n",
        "            # Add contributions from neighbors\n",
        "            for neighbor, connection_strength in graph[word].items():\n",
        "                neighbor_total_connections = sum(graph[neighbor].values())\n",
        "                if neighbor_total_connections > 0:\n",
        "                    contribution = (connection_strength / neighbor_total_connections) * scores[neighbor]\n",
        "                    score += damping * contribution\n",
        "\n",
        "            new_scores[word] = score\n",
        "\n",
        "        scores = new_scores\n",
        "\n",
        "    return scores\n",
        "\n",
        "def count_word_frequencies_per_document(documents):\n",
        "    \"\"\"Build a per-document frequency map: term -> {doc_id: count}.\"\"\"\n",
        "    word_doc_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for doc_id, document in enumerate(documents, 1):\n",
        "        words = clean_text(document)\n",
        "\n",
        "        for word in words:\n",
        "            word_doc_counts[word][doc_id] += 1\n",
        "\n",
        "    return word_doc_counts\n",
        "\n",
        "def create_firebase_structure(top_keywords, word_doc_counts):\n",
        "    \"\"\"Create the RTDB structure expected by your search engine: keyword -> DocsIds map + term field.\"\"\"\n",
        "    firebase_data = {}\n",
        "\n",
        "    for keyword in top_keywords:\n",
        "        firebase_data[keyword] = {\n",
        "            \"DocsIds\": {},\n",
        "            \"term\": keyword\n",
        "        }\n",
        "\n",
        "        # Store doc-id as string because RTDB keys are strings\n",
        "        doc_counts = word_doc_counts[keyword]\n",
        "        for doc_id, count in doc_counts.items():\n",
        "            firebase_data[keyword][\"DocsIds\"][str(doc_id)] = count\n",
        "\n",
        "    return firebase_data\n",
        "\n",
        "def upload_to_firebase(data):\n",
        "    \"\"\"Upload the entire inverted index to Firebase RTDB (overwrites the node).\"\"\"\n",
        "    try:\n",
        "        response = requests.put(FIREBASE_URL_PLANET, json=data)\n",
        "\n",
        "        if response.status_code in [200, 201]:\n",
        "            print(\"Successfully uploaded to Firebase\")\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"Upload failed with status: {response.status_code}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"Upload error: {e}\")\n",
        "        return False\n",
        "\n",
        "def extract_textrank_keywords(documents, top_k=30):\n",
        "    \"\"\"Extract top keywords from the whole corpus using TextRank.\"\"\"\n",
        "    combined_text = \" \".join(documents)\n",
        "    words = clean_text(combined_text)\n",
        "\n",
        "    # If we barely have tokens, just return what we have\n",
        "    if len(words) < 10:\n",
        "        return words[:top_k]\n",
        "\n",
        "    graph = build_word_graph(words)\n",
        "    scores = calculate_textrank_scores(graph)\n",
        "\n",
        "    ranked_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Keep only the term strings\n",
        "    keywords = [word for word, score in ranked_words[:top_k]]\n",
        "    return keywords\n",
        "\n",
        "def run_textrank_with_firebase():\n",
        "    \"\"\"Run TextRank on multiple papers, print results, and upload the inverted index to Firebase.\"\"\"\n",
        "    urls = [\n",
        "        \"https://link.springer.com/article/10.1007/s13593-014-0246-1\",\n",
        "        \"https://link.springer.com/chapter/10.1007/978-981-15-6315-7_23\",\n",
        "        \"https://link.springer.com/chapter/10.1007/978-981-15-5959-4_11\",\n",
        "        \"https://link.springer.com/chapter/10.1007/978-981-15-2774-6_5\",\n",
        "        \"https://link.springer.com/chapter/10.1007/978-981-15-6315-7_24\",\n",
        "    ]\n",
        "\n",
        "    documents = []\n",
        "    successful_urls = 0\n",
        "\n",
        "    # Fetch each URL and extract only the parts we care about\n",
        "    for i, url in enumerate(urls, 1):\n",
        "        soup = fetch_page(url)\n",
        "\n",
        "        if soup:\n",
        "            content = extract_content(soup)\n",
        "            if content:\n",
        "                documents.append(content)\n",
        "                successful_urls += 1\n",
        "\n",
        "    if not documents:\n",
        "        print(\"No documents processed successfully\")\n",
        "        return None\n",
        "\n",
        "    print(f\"Total documents processed: {successful_urls}\")\n",
        "\n",
        "    keywords = extract_textrank_keywords(documents, top_k=30)\n",
        "    word_doc_counts = count_word_frequencies_per_document(documents)\n",
        "    firebase_data = create_firebase_structure(keywords, word_doc_counts)\n",
        "\n",
        "    print(\"\\nTOP 30 TEXTRANK KEYWORDS:\")\n",
        "    print(\"=\" * 40)\n",
        "    for i, keyword in enumerate(keywords, 1):\n",
        "        print(f\"{i:2d}. {keyword}\")\n",
        "\n",
        "        # Print how the keyword is distributed across docs\n",
        "        doc_counts = word_doc_counts[keyword]\n",
        "        if doc_counts:\n",
        "            docs_info = [f\"Doc{doc_id}:{count}\" for doc_id, count in sorted(doc_counts.items())]\n",
        "            print(f\"     {', '.join(docs_info)}\")\n",
        "\n",
        "    success = upload_to_firebase(firebase_data)\n",
        "\n",
        "    return {\n",
        "        'keywords': keywords,\n",
        "        'documents': documents,\n",
        "        'firebase_data': firebase_data,\n",
        "        'word_doc_counts': word_doc_counts\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = run_textrank_with_firebase()\n",
        "\n",
        "    if results:\n",
        "        print(\"\\nTextRank extraction and Firebase upload completed\")\n",
        "    else:\n",
        "        print(\"\\nExtraction failed\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzqK4cGMve93",
        "outputId": "e5cc2285-bef0-43e7-97e8-05492a1d2226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documents processed: 5\n",
            "\n",
            "TOP 30 TEXTRANK KEYWORDS:\n",
            "========================================\n",
            " 1. diseas\n",
            "     Doc1:6, Doc2:6\n",
            " 2. detect\n",
            "     Doc1:4, Doc2:2, Doc4:1, Doc5:3\n",
            " 3. monitor\n",
            "     Doc1:1, Doc4:2, Doc5:6\n",
            " 4. flood\n",
            "     Doc5:9\n",
            " 5. techniqu\n",
            "     Doc1:3, Doc2:1, Doc4:2, Doc5:2\n",
            " 6. imag\n",
            "     Doc2:2, Doc5:6\n",
            " 7. plant\n",
            "     Doc1:4, Doc2:4\n",
            " 8. technolog\n",
            "     Doc1:1, Doc3:7\n",
            " 9. commun\n",
            "     Doc3:8\n",
            "10. iot\n",
            "     Doc3:8\n",
            "11. sens\n",
            "     Doc1:2, Doc4:3, Doc5:2\n",
            "12. learn\n",
            "     Doc2:2, Doc5:4\n",
            "13. develop\n",
            "     Doc1:1, Doc3:4\n",
            "14. product\n",
            "     Doc2:3, Doc3:1\n",
            "15. infect\n",
            "     Doc1:4\n",
            "16. network\n",
            "     Doc2:3, Doc5:1\n",
            "17. help\n",
            "     Doc1:2, Doc2:1, Doc5:1\n",
            "18. disast\n",
            "     Doc2:1, Doc5:3\n",
            "19. serolog\n",
            "     Doc1:4\n",
            "20. deep\n",
            "     Doc5:4\n",
            "21. process\n",
            "     Doc1:1, Doc4:2\n",
            "22. agricultur\n",
            "     Doc1:2, Doc2:1\n",
            "23. day\n",
            "     Doc1:1, Doc4:1, Doc5:1\n",
            "24. innov\n",
            "     Doc1:2, Doc3:1\n",
            "25. optic\n",
            "     Doc4:3\n",
            "26. highli\n",
            "     Doc2:2, Doc4:1\n",
            "27. machin\n",
            "     Doc2:3\n",
            "28. leaf\n",
            "     Doc2:3\n",
            "29. india\n",
            "     Doc2:1, Doc5:2\n",
            "30. crop\n",
            "     Doc1:1, Doc2:2\n",
            "Successfully uploaded to Firebase\n",
            "\n",
            "TextRank extraction and Firebase upload completed\n"
          ]
        }
      ]
    }
  ]
}
